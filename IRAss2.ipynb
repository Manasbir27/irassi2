{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38f86c8-ce0d-4040-8e5d-c70611b85525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 125\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# To run the search system, uncomment the line below\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 110\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m searcher \u001b[38;5;241m=\u001b[39m Searcher(indexer)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your search query (or type \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to quit): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:1261\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py:1304\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Preprocessing functions\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Tokenizes, removes stop words, punctuations, and stems the words.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Step 1: Indexing the corpus\n",
    "class Indexer:\n",
    "    def __init__(self):\n",
    "        self.dictionary = defaultdict(lambda: {'df': 0, 'postings': []})\n",
    "        self.doc_lengths = {}  # Store document length (for normalization)\n",
    "        self.N = 0  # Total number of documents\n",
    "\n",
    "    def index_document(self, doc_name, content):\n",
    "        \"\"\"Indexes a single document by file name.\"\"\"\n",
    "        term_freq = defaultdict(int)\n",
    "        tokens = preprocess(content)\n",
    "        \n",
    "        # Count term frequencies in the document\n",
    "        for token in tokens:\n",
    "            term_freq[token] += 1\n",
    "        \n",
    "        # Update the dictionary and document frequency\n",
    "        for term, freq in term_freq.items():\n",
    "            if len(self.dictionary[term]['postings']) == 0 or self.dictionary[term]['postings'][-1][0] != doc_name:\n",
    "                self.dictionary[term]['df'] += 1\n",
    "            self.dictionary[term]['postings'].append((doc_name, freq))\n",
    "        \n",
    "        # Compute document length for normalization\n",
    "        self.doc_lengths[doc_name] = self.compute_doc_length(term_freq)\n",
    "        self.N += 1\n",
    "\n",
    "    def compute_doc_length(self, term_freq):\n",
    "        \"\"\"Computes the length of a document for normalization.\"\"\"\n",
    "        length = 0\n",
    "        for freq in term_freq.values():\n",
    "            length += (1 + math.log10(freq))**2\n",
    "        return math.sqrt(length)\n",
    "\n",
    "    def build_index(self, folder_path):\n",
    "        \"\"\"Indexes all documents in the folder.\"\"\"\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    self.index_document(filename, content)\n",
    "\n",
    "# Step 2: Ranked Retrieval (searching with cosine similarity)\n",
    "class Searcher:\n",
    "    def __init__(self, indexer):\n",
    "        self.indexer = indexer\n",
    "\n",
    "    def search(self, query):\n",
    "        \"\"\"Searches for the top 10 relevant documents based on the query.\"\"\"\n",
    "        query_tokens = preprocess(query)\n",
    "        query_vector = self.compute_query_vector(query_tokens)\n",
    "        scores = defaultdict(float)\n",
    "\n",
    "        # Calculate cosine similarity for each document\n",
    "        for term, weight in query_vector.items():\n",
    "            if term in self.indexer.dictionary:\n",
    "                postings = self.indexer.dictionary[term]['postings']\n",
    "                idf = math.log10(self.indexer.N / self.indexer.dictionary[term]['df'])\n",
    "                for doc_name, tf in postings:\n",
    "                    tf_weight = 1 + math.log10(tf)\n",
    "                    scores[doc_name] += tf_weight * weight * idf\n",
    "\n",
    "        # Normalize by document lengths\n",
    "        for doc_name in scores:\n",
    "            scores[doc_name] /= self.indexer.doc_lengths[doc_name]\n",
    "\n",
    "        # Sort by score and return top 10 results\n",
    "        ranked_docs = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "        return ranked_docs[:10]\n",
    "\n",
    "    def compute_query_vector(self, query_tokens):\n",
    "        \"\"\"Computes the tf-idf vector for the query using ltc weighting.\"\"\"\n",
    "        term_freq = defaultdict(int)\n",
    "        for token in query_tokens:\n",
    "            term_freq[token] += 1\n",
    "\n",
    "        query_vector = {}\n",
    "        for term, freq in term_freq.items():\n",
    "            tf_weight = 1 + math.log10(freq)\n",
    "            idf_weight = math.log10(self.indexer.N / (self.indexer.dictionary[term]['df'] if term in self.indexer.dictionary else 1))\n",
    "            query_vector[term] = tf_weight * idf_weight\n",
    "        return query_vector\n",
    "\n",
    "# Main driver function\n",
    "def main():\n",
    "    folder_path = \"D:/IR/Corpus-20230203T210935Z-001/Corpus\"  # Hardcoded path to the corpus folder\n",
    "    indexer = Indexer()\n",
    "    indexer.build_index(folder_path)\n",
    "    searcher = Searcher(indexer)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Enter your search query (or type 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        results = searcher.search(query)\n",
    "\n",
    "        print(\"\\nTop 10 relevant documents:\")\n",
    "        if results:\n",
    "            for doc_name, score in results:\n",
    "                print(f\"{doc_name}: Score {score:.10f}\")\n",
    "        else:\n",
    "            print(\"No relevant documents found.\")\n",
    "\n",
    "# To run the search system, uncomment the line below\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db05ce1-02f0-42f8-9112-c04f7e66cbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8b693-fc2b-49e5-8e3c-912680582955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
